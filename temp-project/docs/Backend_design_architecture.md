Technical Architecture & Backend Design
To deliver the above features with high performance and scalability, HNNT will employ a modern, serverless-friendly tech stack. The focus is on using robust managed services to achieve low latency and handle viral growth without major infrastructure headaches. Below is an overview of the architecture components and how data will flow: Frontend:
The app will be built with React Native (Expo) for iOS and Android. This allows us to develop the UI once and deploy across both platforms, ensuring a consistent experience for our users on the App Store and Google Play. Expo will streamline usage of native SDKs like Snap Kit and allow quick iteration. React Native is well-suited for our use case – many social apps use it for fast development, and we can achieve 60fps smooth interactions for list dragging, etc.
We will integrate Snap Kit SDK, TikTok SDK, and Google OAuth in the React Native app. Snap Kit’s Login Kit will handle Snapchat authentication tokens
wired.com
, and similarly for TikTok. The app will securely send these tokens to our backend to verify and create a user account (likely using AWS Cognito or a custom auth lambda with JWT issuance).
Expo modules will allow access to phone contacts (for invite flow), camera (if we let user take a photo for profile or for a situationship avatar), and push notifications.
The UI will be implemented using React Native components; for smooth drag-and-drop ranking, we might use a library like Reanimated or DraggableFlatList. For chat interface, a library like GiftedChat can be leveraged and customized.
The frontend will communicate with the backend primarily via GraphQL API calls (and subscriptions for real-time events), using Apollo Client or AWS Amplify’s client libraries.
Backend:
We plan to use AWS Amplify to configure and deploy our backend services. Amplify (which uses AWS AppSync for GraphQL, AWS Cognito for auth, DynamoDB for databases, etc.) allows us to rapidly set up a cloud backend that is secure, scalable, and real-time out of the box
dev.to
dev.to
. Amplify’s CLI will help define a GraphQL schema for our data models, and it will automatically generate the resolvers and database tables. This significantly speeds up development and ensures we can scale without a lot of DevOps work. With Amplify, we can “quickly and easily create APIs, databases, and storage” for the app
dev.to
, and we get built-in support for real-time updates via GraphQL subscriptions
dev.to
, which is useful for friend voting notifications.
GraphQL API: All app data interactions will be through a GraphQL API, which provides a flexible and network-efficient way to query data. GraphQL allows the app to request exactly the fields it needs and nothing more, avoiding the over-fetching problem often seen with REST
hygraph.com
. For example, when loading the friend feedback screen, the app can ask for just the list of situationships and vote counts, without fetching the entire profile or chat history. This reduces bandwidth and latency for mobile users, improving performance. The GraphQL schema will define types such as User, Situationship, Vote, FeedbackComment, etc., and relationships between them. It will also define queries and mutations like createSituationship, rankSituationships, submitVote, etc.
Data Storage: We will likely use AWS DynamoDB (via Amplify DataStore) as the primary database for user data, given its scalability and low-latency reads/writes (important for a snappy social app). Each type in the GraphQL schema can map to a DynamoDB table or a single-table design. For example:
User: stores basic profile info (id, username, Snap or TikTok ID for login linkage, profile avatar URL, settings like isPrivate, etc.).
Situationship: stores entries (id, userId (owner), name/nickname, category/tag, order/rank position, maybe a short description or note). If we allow an optional photo, an S3 bucket via Amplify Storage will hold the image and the URL is referenced here.
Rankings: We might not need a separate model if rank is just an order field on the Situationship, but we could have a list order stored (maybe the Situationship model has a numeric rankIndex).
Vote/Feedback: stores votes or feedback per friend per situationship list. We might model the whole voting on a list as one entity, or individual votes. For simplicity, perhaps a Vote type with (id, voterUserId, targetUserId (the owner of list), chosenSituationshipId, voteType [e.g. BEST or WORST], optional comment). Or separate BestVote and WorstVote. Alternatively, we aggregate results and store just counts on each situationship (but keeping individual votes lets more transparency and possibly reveal who voted what if not anonymous).
Friend Relationships: We will use either a mapping of followers in the database or leverage social logins to connect friends. Likely simpler to let users add friends by username or by discovering contacts. A Friend join table (or using Amplify’s multi-auth owner array feature for shareable data) could list which users can view which profile. Amplify’s GraphQL supports fine-grained auth, so we can mark a Situationship list as owned by the user and shareable to certain friend IDs, implementing the “mutuals only” rule easily (Amplify now even supports multi-owner models where you can specify an array of user IDs who have access)
aws.amazon.com
.
Reports: A simple table for user reports (id, reportedBy, reportedUserOrContentId, reason, timestamp) for moderation.
Authentication & Authorization: AWS Cognito (via Amplify Auth) will manage user accounts. When users sign in with Snapchat, etc., we exchange the OAuth token to either log in or create a new Cognito user behind the scenes. Cognito will give us unique user IDs and JWT tokens for client auth. We will attach user roles/claims in JWT such that our GraphQL AppSync resolvers can enforce that, e.g., users can only modify their own profile or situationships, and only invited friends can query certain data. Public profiles might have a flag that allows certain data (like username, profile pic) to be queried by anyone. Private content (like the list details) will require auth checks. Amplify’s declarative @auth rules in the schema will help enforce these without custom code in many cases (e.g., @auth(rules: [{ allow: owner }, { allow: groups, groupsField: "sharedWith" }]) to allow the owner and specific shared friends).
AI Chat Service: For the AI coaching feature, we will integrate with OpenAI’s API for LLM (likely GPT-4 for quality, with GPT-3.5 as a fallback for cost). However, to meet the low-latency requirement and scale under potentially heavy usage, we plan to use Baseten as an AI backend service. Baseten allows deployment of models and offers an OpenAI-compatible API bridge
sdk.vercel.ai
, meaning we can route requests to either OpenAI or a fine-tuned model hosted on Baseten with minimal changes. In early stages, we might use OpenAI directly (to leverage its stability and safety features), and later, as we refine the prompts and possibly gather conversation data (carefully, with privacy in mind), we could fine-tune a smaller model or use an open-source alternative (like GPT-J or others) hosted on Baseten to reduce costs. Baseten’s platform ensures high-performance inference and scaling, boasting enterprise-level reliability (e.g., SOC2, high uptime)
baseten.co
, which aligns with our need for the AI coach to be always available. The AI Chat API in our system will work like:
The app calls our GraphQL mutation or a direct endpoint (if using a separate function) like startAiChat or sendMessageToAI(situationshipId, message).
The backend (likely an AWS Lambda function) will package the user’s query along with some context (we can include a brief summary of the situationship if available, or previous Q&A from that thread) and call the Baseten endpoint (or OpenAI).
We’ll use streaming responses if possible to show the answer typing out in the app for better UX. Baseten supports streaming proxies for OpenAI calls as well.
We also log the conversation (with user consent in terms) to improve the AI and also to have a record in case of abuse or content issues.
The AI Chat API will also incorporate content moderation: OpenAI’s models themselves have moderation filters. We can double-check the response using OpenAI’s Moderation API or a similar service to ensure no harmful or disallowed content is returned to the user. If something is flagged (e.g., a user asks for self-harm advice or extremely explicit content), the system can either refuse or provide a safe completion according to our policy.
Realtime Updates: Certain interactions benefit from realtime updates. For example, if a friend is on the vote page and the user reorders her list, or when a friend submits a vote, the user (if online in the app) should see the new vote without needing to refresh. AWS AppSync (GraphQL) offers subscriptions which can push data to clients when mutations occur. We will utilize this for:
Votes/Feedback: Subscribe the list owner to votes on her situationships so she gets immediate feedback.
Perhaps AI typing indicator: We might not need GraphQL for that if using direct WebSocket to stream, but we could send an event when AI is done processing to notify the client.
Online status or typing of friends is not critical here since it’s not a two-way chat app (besides AI).
APIs Documentation: The following summarizes the key APIs and their roles:
Auth APIs: Endpoints (backed by Cognito/SnapKit) for logging in via Snapchat, TikTok, or Google. On login, if new, create user profile with info from OAuth (e.g., Snapchat gives display name and Bitmoji avatar if permitted). Ensure the OAuth process handles token verification securely. Possibly a separate API to link additional accounts (so a user can connect both Snapchat and TikTok for easier friend finding).
User Profile API: GraphQL queries and mutations to get or update profile info. E.g., getUser(id), updateUser(input). This handles profile visibility settings, username changes, etc. It will enforce that one can only update their own profile (unless admin). Might also include a query like searchUsers(username) if we allow adding friends by username.
Situationship Management API: Mutations like createSituationship(name, category, [photo]), updateSituationship(id, fields), deleteSituationship(id), and perhaps listSituationships(userId) for retrieving a user’s list. This will include the position/rank field. For ranking, we could have a dedicated mutation reorderSituationships(idsInOrder: [ID]!) which the client calls after a drag-and-drop reorder, sending the new sorted list of IDs – the backend will then update each item’s rank index accordingly in one transaction. This keeps the logic simple and on the client side for ordering.
Ranking/Sharing API: (Could be merged with above) – Possibly a convenience query getSituationshipListForShare(userId) that returns the list and maybe a short-lived URL or code to fetch an image for sharing. If we generate images server-side for sharing, an API would handle that (or the client can compose it locally).
Voting/Feedback API: Mutations for friends to submit feedback, e.g., submitVote(targetUserId, bestChoiceId, worstChoiceId, comment) or similar. This will create vote records linked to the target user’s list. There could also be a query getFriendFeedback(targetUserId) that a friend (who has permission) or the user herself uses to retrieve the aggregated results (though the user could just query her own situationships which now have updated vote counts). If anonymity is off, the vote records will include voter’s userId so the owner can see who voted what; if anonymity is on, we might still store the voter but mark it hidden – or we issue the friend a one-time anonymous token for that vote submission so it’s not linked to their userId in the query that the owner sees. We will design this to honor the chosen privacy mode. Additionally, a listInvitableFriends API could help the user see who (from contacts or social connections) is already on the app to invite or who might need an SMS invite.
AI Chat API: Perhaps not a GraphQL but a direct HTTP endpoint (for streaming ease) like /ai/query. However, we can also model it in GraphQL by having a Message type and using subscriptions for responses. Simpler: the app hits an API Gateway endpoint with the user’s message and receives a stream or polling ID. For now, we describe it as: askAI(situationshipId, messageText): AIResponse. The implementation calls the AI model as described. This API will also use an LLM system prompt to keep the AI focused (we will craft a prompt that instructs it to act as a wise, supportive friend and relationship coach, with context about the user’s situation if available).
Block/Report API: Mutations like blockUser(targetUserId) and reportUser(targetUserId, reason) or reportContent(contentId, reason). Blocking will update a relationship table (or a block list attribute on the user). Reporting will create a report entry and possibly trigger an email/alert to admins. We will also expose queries for admins/moderators (internal, not to regular users) to fetch reported content and user info for review. This ensures moderation capabilities as usage grows.
Payment API: While much of the in-app purchase will be handled by Apple/Google SDKs on the client side, we will need backend verification. We can use RevenueCat or simply use our own endpoint to verify receipts. E.g., verifyPurchase(receipt) which our server calls Apple/Google servers to validate and then updates the user’s subscription status in the database (or Cognito attributes). For one-time purchases, similar flow or unlock a feature flag on the user. If using Stripe for any web payments (less likely initially), we’d have webhook handling to grant features. We’ll maintain an Entitlements table or use Cognito user attributes to mark who is premium or who owns which add-on, so the app can check and unlock features accordingly.
Scalability Considerations: All chosen components are cloud-native and scale-ready. AWS AppSync (GraphQL) and Lambda can scale to millions of users as needed, and DynamoDB can handle high request volumes with low latency. Using serverless architecture means we pay per usage and can handle viral spikes without crashing. Given the ambition for high K-factor virality (like Gas had), we anticipate scenarios of rapid user influx. We will employ caching where possible: for example, friend suggestions or common queries could be cached in AppSync or a CDN layer. But much data is user-specific and fast-moving, so DynamoDB’s performance suffices. For media (profile pics, shared images), we use Amazon S3 with CloudFront CDN in front, so any images (like the shareable snapshot of a list) load quickly worldwide. If the app creates dynamic images for sharing, we might pre-generate and cache them on S3 or use an on-demand image generation service.
Latency Optimization: Every interaction should feel snappy. Some strategies:
Use GraphQL subscriptions or webhooks to push updates instead of requiring the client to poll (e.g., the friend’s vote appears instantly).
Leverage the fact that GraphQL can fetch exactly needed data to minimize payload sizes, especially important on cellular networks
hygraph.com
.
Keep the AI chat as fast as possible: possibly use a smaller model for quick responses to trivial queries and reserve the bigger model for complex ones, or use streaming as mentioned.
Use edge locations for serving static content (via CDN) and perhaps regional endpoints if we have international users (initially focus is Seattle, but as we expand, deploying the backend to multiple AWS regions or using AWS Global Accelerator can reduce latency).
The React Native app will use local state management to immediately reflect UI changes (optimistic updates). For example, when the user reorders her list, the UI should update immediately rather than waiting for server confirmation (we’ll still sync to server in background).
Analytics & Logging: We will integrate analytics (perhaps Amplify Analytics or a tool like Segment/Amplitude) to track user behavior: how many situationships are added, frequency of AI chat usage, conversion to sharing, etc. This data will guide improvements and also help fine-tune the paywall (as per the requirement to analyze engagement for optimal paywalls). We’ll also log errors and performance metrics (using CloudWatch or Sentry) to quickly catch issues, especially around AI responses or any latency bottlenecks.
In summary, the technology stack (Expo RN + AWS Amplify + GraphQL + LLM via Baseten/OpenAI) is chosen to maximize development speed and scalability. It allows the small development team to focus on product iteration and user experience, while relying on cloud services to handle the heavy lifting of authentication, real-time data sync, and AI model serving. This is in line with modern best practices – using managed backend services means we “quickly and easily build an application that is secure, scalable, and reliable”
dev.to
 without reinventing the wheel on infrastructure.
